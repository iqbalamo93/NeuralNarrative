---
title: 01 Transformers
description: A guide in my new Starlight docs site.
---

import '../../../../styles/global.css';  
import myImage from "./transformer.png";
import React from 'react';
import Draggable from 'react-draggable';
import ff_image from "./ff.png";
import { Image } from 'astro:assets';

#### Crafting the Transformer: An Object-Oriented Approach

In this section, we're going to construct a Transformer model from the ground up. Our methodology is inspired by the insights of Umar Jamil—be sure to check out his work for a deeper understanding.

To bring the Transformer to life, we've broken down the development process into two primary segments: modeling and training. We'll start with modeling, which is crucial to grasp before we proceed to the training phase. As a constant guide, we've placed an image of the Transformer architecture in the right corner of the page for your reference.

Within our `model.py`, we've crafted a robust framework composed of nine classes and one essential function. These components form the backbone of our implementation:
<Draggable>
<div class="fixed-image">
    <Image src={myImage} width="300" alt="A description of my image." />
</div>
</Draggable>

## Table of Contents

- [Input Embeddings](#input-embeddings)
- [Positional Encoding](#positional-encoding)
- [Layer Normalization](#layer-normalization)
- [Feed Forward Block](#feed-forward-block)
- [Multi-Head Attention Block](#multi-head-attention-block)
- [Residual Connection](#residual-connection)
- [Encoder Block](#encoder-block)
- [Encoder](#encoder)
- [Decoder Block](#decoder-block)
- [Decoder](#decoder)
- [Projection Layer](#projection-layer)
- [Transformer](#transformer)

The function buildTransformer weaves these components together, initializing the architecture for our model.

Understanding the Architecture
In the forthcoming sections, we'll dissect how each of these twelve classes contributes to a functioning Transformer model. It's important to recognize the role of each component:

**Classes:** They represent distinct, well-defined parts of our model, encapsulating specific functionalities.
Object-Oriented Design: This paradigm ensures our code is modular, making it easier to understand, maintain, and extend.
By the end of this exploration, you'll have a comprehensive understanding of the nuts and bolts of the Transformer model. Stay tuned as we delve into the intricacies of each class and function, paving the way towards a robust implementation.

`Remember, the full codebase on GitHub will offer a more granular look at the inner workings of the model. This guide aims to provide a high-level understanding, ensuring you grasp the architectural decisions and algorithmic flow that define the Transformer.`
<Fragment>
<div class="content">
## Input Embeddings

```python
class InputEmbeddings(nn.Module):
    def __init__(self, d_model:int , vocab_size:int ):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)
```
     `d_model: int` - The dimensionality of the embedding space.  
     `vocab_size: int` - The size of the vocabulary of source language.  
     `Forward Method`- Accepts input `x` and produces embeddings scaled by the square root of `d_model`, leading to an output size of `(vocab_size, d_model)`.
   
   **Notes**
    > The embedding layer weights are scaled by the square root of `d_model` as suggested in the original paper.
</div>
</Fragment>

<Fragment>
## Positional Encoding
```python
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, seq_len: int , dropout: float) ->None:
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(seq_len,d_model)
        #create a vector of shape (seq_len,1)
        position = torch.arange(0,seq_len, dtype = torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))
        #Apply the sin to even position
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe',pe)

    def forward(self,x):
        x = x+(self.pe[:,:x.shape[1],:]).requires_grad_(False) # (batch, seq_len, d_model)

        return self.dropout(x)
```
The `PositionalEncoding` class extends `nn.Module` and is designed to provide each token in a sequence with a unique position encoding. Here's a breakdown of its implementation and purpose:

- **Constructor (`__init__` method)**
  - Initializes the positional encoding with the following parameters:
    - `d_model` (int): Represents the dimensionality of the embedding space.
    - `seq_len` (int): The maximum length of the sequence to be encoded.
    - `dropout` (float): The dropout rate for regularization.

Within the constructor:

- A zero matrix `pe` of shape `(seq_len, d_model)` is created to store the positional encodings.
- The `position` tensor is generated with values from `0` to `seq_len-1` and reshaped by `unsqueeze(1)` to have a shape of `(seq_len, 1)`. This function is used to add a dimension, making matrix operations possible.
- `div_term` calculates a divisor used in the alternating sine and cosine functions based on the model's dimension and a fixed constant (10000.0). The `exp` and `arange` functions create values for this divisor.
- Sine is applied to even indices in the positional encoding matrix, while cosine is applied to odd indices. This alternation provides a unique pattern for each position.
- The `unsqueeze(0)` is used on `pe` to add a batch dimension, making it compatible with the expected input dimensions.

- **Register Buffer**
  - The `register_buffer` method is used to create a persistent, non-learnable buffer for the positional encoding tensor `pe`. This buffer is not a parameter of the model and will not be updated during training, but it will be part of the model's state, allowing for easy saving and loading.

- **Forward Method (`forward` method)**
  - In the forward pass, the input `x` is added to the positional encodings, ensuring that each token's position is considered.
  - The positional encodings up to `x.shape[1]` (the sequence length of the batch) are used, and `requires_grad_` is set to `False` to indicate that no gradient should be computed.
  - Finally, dropout is applied to the resulting tensor.


**Important Points:**
- The output tensor maintains the shape `(batch, seq_len, d_model)`, adhering to the expected input dimensions for subsequent layers.
- Keeping track of tensor shapes at each operation is a crucial practice for debugging. It helps ensure the consistency of tensor operations and can prevent shape mismatches, which are common sources of errors.
- The module's design, which includes sinusoidal patterns and a non-learnable buffer, is a deliberate choice to provide the model with an effective way to interpret token positions without increasing the number of trainable parameters.

`Debugging Tip:`Always verify the shape of your tensors after each operation, especially when combining different components like positional encodings with embeddings. This practice can help identify and resolve many issues early in the development process.

</Fragment>

<Fragment>
## Layer Normalization
```python
class LayerNormalization(nn.Module):

    def __init__(self, eps:float = 10**-6)->None:
        super().__init__()

        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(1)) #multipled
        self.bias = nn.Parameter(torch.zeros(1)) #added

    def forward(self, x):
        mean = x.mean(dim = -1, keepdim=True)
        std = x.std(dim = -1, keepdim=True)
        return self.alpha * (x-mean)/(std +self.eps)+self.bias
```
## Class: LayerNormalization

The `LayerNormalization` module is a fundamental part of modern neural networks that stabilizes the activation distribution throughout the training process. Below is the detail of its structure and operations:

- **Constructor (`__init__` method)**
  - Initializes the layer normalization with an epsilon value `eps` for numerical stability:
    - `eps` (float, default: `1e-6`): A small constant added to the standard deviation to prevent division by zero.

  Inside the constructor:
  
  - `self.alpha` is a learnable scaling parameter, initialized to one. It's responsible for scaling the normalized data.
  - `self.bias` is a learnable shifting parameter, initialized to zero. It allows the layer to shift the normalized data if needed.
  - Both `self.alpha` and `self.bias` are defined as `nn.Parameter` to indicate that they should be considered during the optimization process.

- **Forward Method (`forward` method)**
  - The forward pass computes the mean and standard deviation across the last dimension of the input `x`.
  - It normalizes `x` by subtracting the mean and dividing by the standard deviation, which is offset by `eps` for numerical stability.
  - The learnable parameters `self.alpha` (often denoted as γ in literature) and `self.bias` (often denoted as β) are then applied to the normalized output.
  - This operation ensures that the activations have a mean of zero and a standard deviation of one, and then scales and shifts them as determined by the learned parameters.

**Important Points:**

- `self.alpha` and `self.bias` are key to the layer's ability to adaptively scale and shift the normalized data. This adaptability is crucial for the layer's effectiveness in different contexts within the network.
- The epsilon value `eps` adds stability to the normalization process, ensuring that the division by the standard deviation doesn't lead to extremely large values when the standard deviation is very small.
- The layer normalization process helps to reduce the internal covariate shift, which can significantly speed up training and lead to better overall performance.

**Debugging Tip:**

Keep an eye on the scale of `self.alpha` and the shift of `self.bias` as the training progresses. Unusual values in these parameters could indicate issues with data normalization or the learning rate.

</Fragment>

<Fragment>
  ## Feed Forward Block
 ```python
 class FeedForwardBlock(nn.Module):

    def __init__(self, d_model: int, d_ff:int, dropout:float) -> None:
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff) #W1 and B2
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model) #W2 and B2

    def forward(self, x):
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))
 ```
The `FeedForwardBlock` module is an essential part of the Transformer architecture that performs feed-forward operations this is used in both encoder and decoder. 
Here's an overview of its setup and function:

- **Constructor (`__init__` method)**
    - The block is initialized with the following parameters:
    - `d_model` (int): The number of input and output features from the previous and next layers, respectively.
    - `d_ff` (int): The number of features in the hidden layer, typically larger than `d_model`.
    - `dropout` (float): The dropout rate to regulate the model's complexity and prevent overfitting.

  In the constructor:
  - `self.linear_1` and `self.linear_2` are fully connected layers that transform the input data.

<div align="center">
    <Image src={ff_image}  alt="Feedforward Block" />
</div>



  - A `nn.Dropout` layer is included to reduce the risk of overfitting by randomly setting a subset of activations to zero during training.

- **Forward Method (`forward` method)**
  - The forward pass applies a linear transformation, followed by a ReLU activation function.
  - The `dropout` is applied after the activation function to the intermediate results.
  - Another linear transformation is applied to project the data back to the dimensionality of `d_model`.

**Important Points:**

- The feedforward network consists of two linear transformations with a ReLU activation in between, which helps the network to learn complex mappings between inputs and outputs.
- The dropout layer is essential for the network to generalize well and not just memorize the training data.

**Debugging Tip:**
Monitoring the output after the ReLU activation and before the final linear layer can provide insights into the health of the activations within the network. If most values are zeros, it might indicate dying ReLU problems.

</Fragment>

<Fragment>
  ## Multi-Head Attention Block
  ```python
  class MultiHeadAttentionBlock(nn.Module):

    def __init__(self,d_model: int,h:int, dropout:float) ->None:
        super().__init__()
        self.d_model =d_model
        self.h = h
        assert d_model % h == 0, "d_model is not divisible by h"

        self.d_k = d_model//h 

        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)

        self.w_o = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
    
    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]
        # Just apply the formula from the paper
        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)
        # return attention scores which can be used for visualization
        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        query = self.w_q(q) #(batch, Seq_Len , d_model)--> (Batch, seq_Len, d_model)
        key = self.w_k(k)   #(batch, Seq_Len , d_model)--> (Batch, seq_Len, d_model)
        value = self.w_v(v) #(batch, Seq_Len , d_model)--> (Batch, seq_Len, d_model)

        #(Batch, Seq_Len, d_Model)-->(Batch, Seq_Len, h, d_k)-->(Batch, h, seq_len, d_k)
        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)
        key   = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)
        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)

        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)

        #(Batch, h, Seq_Len, d_k) --> (Batch, Seq_len, h, d_k) -->
        x = x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h * self.d_k)

        #(Batch, Seq_Len, d_model) -->(Batch, Seq_Len, d_model)
        return self.w_o(x)
```
The `MultiHeadAttentionBlock` plays a pivotal role in the transformer model, acting as the crux of the attention mechanism. This class encapsulates several key concepts:

- **Static Attention Method**: 
  - The `attention` function is defined as a `staticmethod`, highlighting a strategic design choice. This allows the method to be utilized across various components of the transformer, including both the encoder and decoder, without the necessity of an instance of the class.
  - It computes attention scores using the transformer's original formula, scaling the dot product of queries and keys by the square root of the keys' dimension (`d_k`).

- **Masking in Attention**: 
  - The method is designed to handle masking, essential for controlling attention flow, like ignoring padding tokens or maintaining causality in the decoder.

- **Dropout for Regularization**: 
  - Incorporating dropout within the attention mechanism aids in preventing overfitting, thus enhancing the model's generalization capability.

- **Reshaping for Multi-Head Attention**: 
  - A distinctive feature of this implementation is the reshaping of the query (`q`), key (`k`), and value (`v`) into 4-dimensional tensors. This facilitates the multi-head attention mechanism, where the model concurrently processes the inputs through multiple attention 'heads'.
  - This reshaping and transposition enable the model to focus on different aspects of the input sequence across various representation subspaces, a hallmark of the transformer's architecture.

- **Usage in Transformer**: 
  - The multi-head attention mechanism is employed thrice in the transformer architecture – twice in the decoder and once in the encoder. This repeated usage underscores its significance in the model's ability to process and interpret sequential data effectively.

- **Final Output Transformation**: 
  - Post attention computation and head concatenation, a final linear transformation through `self.w_o` aligns the output back to the original input dimension (`d_model`), a common practice in transformer models.

This class exemplifies the intricate and efficacious multi-head attention mechanism, pivotal to the transformer's capacity for handling complex sequence-to-sequence tasks. The implementation of a static attention function, along with the strategy for multi-head attention via tensor reshaping, showcases the elegant and proficient design choices inherent in this model.

</Fragment>

<Fragment>
## Residual Connection
```python
class ResidualConnection(nn.Module):

    def __init__(self, dropout: float)->None:
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.norm = LayerNormalization()

    def forward(self, x, sublayer):
        return x +self.dropout(sublayer(self.norm(x)))
```
The `ResidualConnection` module is an integral component of the transformer architecture, playing a pivotal role in stabilizing and enhancing the training process. Let's delve into its structure and functionality:

- **Constructor (`__init__` method)**
  - This method sets up the residual connection with a dropout rate:
    - `dropout` (float): Specifies the dropout rate, a technique used to prevent overfitting by randomly dropping units (neural network nodes) from the computation during training.

  Inside the constructor:
  
  - The `nn.Dropout` layer is initialized with the specified dropout rate, providing a mechanism for regularizing the model.
  - A `LayerNormalization` layer is also instantiated, which normalizes the input layer by layer, ensuring that the network's inputs are normalized in a consistent way. This helps in stabilizing the learning process.

- **Forward Method (`forward` method)**
  - The forward pass of this module introduces the concept of a residual connection, which is a hallmark of the transformer architecture.
  - It takes two arguments:
    - `x`: The input to the residual connection.
    - `sublayer`: A function representing the sub-layer to be applied to the input. Typically, this could be an attention mechanism or a feed-forward network.
  - The input `x` is first normalized using the layer normalization (`self.norm(x)`), then passed through the `sublayer`.
  - The output of the `sublayer` undergoes dropout and is then added back to the original input `x`. This “residual” addition allows the gradients to flow directly through the network without being hindered by deep layers.

**Important Points:**

- The residual connection (`x + ...`) plays a crucial role in enabling deep transformers to be trained effectively. It allows for direct paths for gradient flow during backpropagation, making it easier to train deeper models.
- By applying layer normalization before the sublayer, the module ensures that the inputs to each sublayer are normalized, contributing to more stable and faster training.
- The use of dropout within the residual connection further regularizes the model, preventing over-reliance on any single part of the network and promoting a more robust learning process.

The design of the `ResidualConnection` class exemplifies key principles in building deep learning architectures, like normalization, residual connections, and regularization, all crucial for training deep and effective models.

</Fragment>

<Fragment>
  <h2 id="encoder-block">Encoder Block</h2>
  {/* TODO: Add initialization parameters, forward method, and comments here */}
</Fragment>

<Fragment>
  <h2 id="encoder">Encoder</h2>
  {/* TODO: Add initialization parameters, forward method, and comments here */}
</Fragment>

<Fragment>
  <h2 id="decoder-block">Decoder Block</h2>
  {/* TODO: Add initialization parameters, forward method, and comments here */}
</Fragment>

<Fragment>
  <h2 id="decoder">Decoder</h2>
  {/* TODO: Add initialization parameters, forward method, and comments here */}
</Fragment>

<Fragment>
  <h2 id="projection-layer">Projection Layer</h2>
  {/* TODO: Add initialization parameters, forward method, and comments here */}
</Fragment>

<Fragment>
  <h2 id="transformer">Transformer</h2>
  {/* TODO: Add initialization parameters, forward method, and comments here */}
</Fragment>