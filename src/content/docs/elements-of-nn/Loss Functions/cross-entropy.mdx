---
title: Cross Entropy
description: A Comprehensive Guide to Understanding and Implementing Cross Entropy in Machine Learning
---



## Introduction

Cross Entropy is a critical concept in the fields of machine learning and deep learning, especially in classification problems. It measures the difference between two probability distributions and is often used as a loss function in neural networks.

## What is Cross Entropy?

Cross Entropy originates from information theory, where it quantifies the amount of information lost or gained in transmitting a message. In machine learning, it measures the difference between the predicted probability distribution and the actual distribution.

### Mathematical Definition

Mathematically, Cross Entropy can be defined as:

$
H(p, q) = -\sum_{x} p(x) \log q(x)
$



where `p` is the true distribution and `q` is the predicted distribution.

## Why Use Cross Entropy?

1. **Effectiveness in Classification Problems:** Cross Entropy is particularly effective in scenarios where we need to measure the performance of a model outputting probabilities.

2. **Sensitivity to Prediction Confidence:** It penalizes incorrect predictions more severely when the model is confident about those predictions.

## Implementing Cross Entropy in Machine Learning

### In Neural Networks

Cross Entropy is commonly used as a loss function in neural networks for classification tasks. When paired with softmax activation function in the output layer, it becomes very effective for multi-class classification.

### Example Code Snippet

```python
import torch.nn as nn

# CrossEntropyLoss in PyTorch
loss = nn.CrossEntropyLoss()
```